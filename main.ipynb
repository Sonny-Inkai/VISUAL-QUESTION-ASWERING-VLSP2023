{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2022 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.data.transforms import RandomResizedCropAndInterpolation\n",
    "from timm.optim.lookahead import Lookahead\n",
    "from timm.data import create_transform\n",
    "from PIL import Image\n",
    "\n",
    "from tasks.randaug import RandomAugment\n",
    "\n",
    "from models.modeling_mplug import BertLMHeadModel, BertModel, BertConfig, FusionModel\n",
    "\n",
    "from transformers import AutoTokenizer, XLMRobertaTokenizer, AdamW, get_scheduler\n",
    "from models.predictor import TextGenerator\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scheduler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEmbedding(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        contain_mask_token=False,\n",
    "        prepend_cls_token=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "        if contain_mask_token:\n",
    "            self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.mask_token = None\n",
    "\n",
    "        if prepend_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.cls_token = None\n",
    "\n",
    "    def num_position_embeddings(self):\n",
    "        if self.cls_token is None:\n",
    "            return self.num_patches\n",
    "        else:\n",
    "            return self.num_patches + 1\n",
    "\n",
    "    def forward(self, x, masked_position=None, **kwargs):\n",
    "        B, C, H, W = x.shape\n",
    "        assert (\n",
    "            H == self.img_size[0] and W == self.img_size[1]\n",
    "        ), f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        if masked_position is not None:\n",
    "            assert self.mask_token is not None\n",
    "            mask_token = self.mask_token.expand(batch_size, seq_len, -1)\n",
    "            w = masked_position.unsqueeze(-1).type_as(mask_token)\n",
    "            x = x * (1 - w) + mask_token * w\n",
    "\n",
    "        if self.cls_token is not None:\n",
    "            cls_tokens = self.cls_token.expand(\n",
    "                batch_size, -1, -1\n",
    "            )  # stole cls_tokens impl from Phil Wang, thanks\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transform(is_train, args):\n",
    "    if is_train:\n",
    "        t = [\n",
    "            RandomResizedCropAndInterpolation(args[\"input_size\"], scale=(0.5, 1.0), interpolation=args[\"train_interpolation\"]), \n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ]\n",
    "        if args[\"randaug\"]:\n",
    "            t.append(\n",
    "                RandomAugment(\n",
    "                    2, 7, isPIL=True, \n",
    "                    augs=[\n",
    "                        'Identity','AutoContrast','Equalize','Brightness','Sharpness', \n",
    "                        'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Rotate', \n",
    "                    ]))\n",
    "        t += [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD), \n",
    "        ]\n",
    "        t = transforms.Compose(t)\n",
    "    else:\n",
    "        t = transforms.Compose([\n",
    "            transforms.Resize((args[\"input_size\"], args[\"input_size\"]), interpolation=3), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)\n",
    "        ])\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        'task': None,\n",
    "        'input_size': 224,\n",
    "        'train_interpolation': 'bicubic',\n",
    "        'randaug': True,\n",
    "        'text_encoder': 'uitnlp/visobert',\n",
    "        'text_decoder': 'uitnlp/visobert',\n",
    "        'vision_width': 768,\n",
    "        'roberta_config': './configs/config_bert.json',\n",
    "        'beam_size': 5,\n",
    "        'min_length': 1,\n",
    "        'max_length': 2,\n",
    "        'start_epoch': 0,\n",
    "        'max_epoch': 10, # default 20\n",
    "        'batch_size': 64, # default 128\n",
    "        'test_batch_size': 128,\n",
    "        'seed': 42,\n",
    "        'lr': 5e-4,\n",
    "        'min_lr': 1e-6,\n",
    "        'warmup_epochs': 1,\n",
    "        'warmup_steps': -1,\n",
    "        'update_freq': 1,\n",
    "        'checkpoint_dir': '/kaggle/working/save_states',\n",
    "        'eos': '[SEP]',\n",
    "        'save_model': '/kaggle/working/model.pth',\n",
    "        'result': '/kaggle/working/result.json',\n",
    "        'opt': 'adamw',\n",
    "        'weight_decay': 0.01,\n",
    "        'opt_eps': 1e-8,\n",
    "        'opt_betas': [0.9, 0.999]\n",
    "        \n",
    "        \n",
    "    }\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEIPLUG(nn.Module):\n",
    "    def __init__(self, tokenizer: None, config: None) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.module_setting(config)\n",
    "        self.visual_encoder = VisionEmbedding(contain_mask_token=True, prepend_cls_token=True)\n",
    "        self.text_encoder = BertModel.from_pretrained(config['text_encoder'], config=self.config_encoder, add_pooling_layer=False)  \n",
    "        self.fusion_encoder = FusionModel.from_pretrained(config['text_encoder'], config=self.config_fusion, add_pooling_layer=False)  \n",
    "        self.text_decoder = BertLMHeadModel.from_pretrained(config['text_decoder'], config=self.config_decoder)    \n",
    "        self.beam_generator = TextGenerator(config, self.text_decoder) \n",
    "\n",
    "    def forward(self, image: None, question: None, answer: None, train: True):\n",
    "        image = image.to(dtype=next(self.parameters()).dtype) \n",
    "        image_embeds = self.visual_encoder(image)\n",
    "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.divice)\n",
    "\n",
    "        if train:\n",
    "            answer_targets = answer.input_ids.masked_fill(answer.input_ids == self.tokenizer.pad_token_id, -100)      \n",
    "            text_output = self.text_encoder(question.input_ids, attention_mask=question.attention_mask, return_dict=True)\n",
    "            text_embeds = text_output.last_hidden_state\n",
    "            fusion_output = self.fusion_encoder(encoder_embeds=text_embeds, \n",
    "                                                attention_mask = question.attention_mask, \n",
    "                                                encoder_hidden_states = image_embeds,\n",
    "                                                encoder_attention_mask = image_atts, return_dict=False)        \n",
    "\n",
    "            image_output, question_output = fusion_output\n",
    "\n",
    "            question_output = torch.cat([image_output, question_output], 1)\n",
    "            merge_text_attention = torch.cat([image_atts, question.attention_mask], 1)\n",
    "\n",
    "            answer_output = self.text_decoder(answer.input_ids, \n",
    "                                                  attention_mask = answer.attention_mask, \n",
    "                                                  encoder_hidden_states = question_output,\n",
    "                                                  encoder_attention_mask = merge_text_attention,                  \n",
    "                                                  labels = answer_targets,\n",
    "                                                  return_dict = True,   \n",
    "                                                  reduction = 'none',\n",
    "                                                 )    \n",
    "            \n",
    "            loss = answer_output.loss\n",
    "            loss = loss.sum()/image.size(0)\n",
    "\n",
    "            return loss\n",
    "\n",
    "        else:\n",
    "            text_output = self.text_encoder(question.input_ids, attention_mask=question.attention_mask,\n",
    "                                                return_dict=True)\n",
    "            text_embeds = text_output.last_hidden_state\n",
    "            fusion_output = self.fusion_encoder(encoder_embeds=text_embeds, \n",
    "                                                attention_mask = question.attention_mask, \n",
    "                                                encoder_hidden_states = image_embeds,\n",
    "                                                encoder_attention_mask = image_atts,                             \n",
    "                                                return_dict = False) \n",
    "            image_output, question_output = fusion_output \n",
    "            question_output = torch.cat([image_output, question_output], 1)\n",
    "            merge_text_attention = torch.cat([image_atts, question.attention_mask], 1)\n",
    "            topk_ids, topk_probs = self.generation(question_output, merge_text_attention) \n",
    "            return topk_ids, topk_probs\n",
    "\n",
    "\n",
    "\n",
    "    def module_setting(self, config):\n",
    "        self.config_encoder = BertConfig.from_json_file(config['roberta_config'])   \n",
    "        self.config_encoder.num_hidden_layers = self.config_encoder.text_encoder_layers\n",
    "        self.config_fusion = BertConfig.from_json_file(config['roberta_config'])   \n",
    "        self.config_decoder = BertConfig.from_json_file(config['roberta_config'])\n",
    "        self.config_decoder.add_cross_attention = True\n",
    "        self.config_decoder.num_hidden_layers = self.config_decoder.text_decode_layers\n",
    "\n",
    "    def generation(self, question_states, question_atts):\n",
    "        encoder_inputs = [question_states, question_atts]\n",
    "        topk_ids, topk_scores = self.beam_generator.translate_batch(encoder_inputs)  \n",
    "        return topk_ids, topk_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_path: None, image_path: None, is_train: bool, config: None) -> None:\n",
    "        super().__init__()\n",
    "        self.is_train = is_train\n",
    "        self.image_transform = build_transform(is_train=is_train, args=config)\n",
    "        self.dataset = json.load(open(data_path, 'r'))\n",
    "        self.question_list = list(self.dataset['annotations'].keys())\n",
    "        self.image_path = image_path\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.question_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            question_id = self.question_list[idx]\n",
    "            set_vqa_dict = self.dataset['annotations'][question_id]            \n",
    "            question = set_vqa_dict['question']\n",
    "            answer = set_vqa_dict['answer']\n",
    "            \n",
    "            # Mapping to get image\n",
    "            image_id = set_vqa_dict['image_id']\n",
    "            image_id_path = self.dataset['images'][str(image_id)]\n",
    "            image_path = self.image_path + \"\\\\\" + image_id_path\n",
    "            image = default_loader(image_path)\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "            return image, question, answer\n",
    "    \n",
    "        else:\n",
    "            question_id = self.question_list[idx]\n",
    "            set_vqa_dict = self.dataset['annotations'][question_id]            \n",
    "            question = set_vqa_dict['question']\n",
    "                    \n",
    "            # Mapping to get image\n",
    "            image_id = set_vqa_dict['image_id']\n",
    "            image_id_path = self.dataset['images'][str(image_id)]\n",
    "            image_path = self.image_path + \"\\\\\" + image_id_path\n",
    "            image = default_loader(image_path)\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "            return image, question, question_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader(dataset: Dataset, is_train: bool, batch_size: int, num_workers: int = 0):\n",
    "    if is_train:\n",
    "        shuffle = True\n",
    "        drop_last = True\n",
    "    else:        \n",
    "        shuffle = False\n",
    "        drop_last = False\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=drop_last\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Accelerator notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \".\\\\data\\\\\"\n",
    "train_dataset = Dataset(data_path + \"training-data\\\\vlsp2023_train_data.json\", \n",
    "                        data_path + \"training-data\\\\training-images\",\n",
    "                        is_train=True, config=config\n",
    "                        )\n",
    "dev_dataset = Dataset(data_path + \"public-test-data\\\\vlsp2023_dev_data.json\", \n",
    "                        data_path + \"public-test-data\\\\dev-images\",\n",
    "                        is_train=False, config=config\n",
    "                        )\n",
    "test_dataset = Dataset(data_path + \"private-test-data\\\\vlsp2023_test_data.json\", \n",
    "                        data_path + \"private-test-data\\\\test-images\",\n",
    "                        is_train=False, config=config\n",
    "                        )\n",
    "\n",
    "train_loader = create_loader(train_dataset, is_train=True, batch_size=config['batch_size'])\n",
    "dev_loader = create_loader(dev_dataset, is_train=False, batch_size=config['batch_size'])\n",
    "test_loader = create_loader(test_dataset, is_train=False, batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(config['text_encoder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scheduler lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0,\n",
    "                     start_warmup_value=0, warmup_steps=-1, sched_type=\"cos\"):\n",
    "    warmup_schedule = np.array([])\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    if warmup_steps > 0:\n",
    "        warmup_iters = warmup_steps\n",
    "    #print(\"Set warmup steps = %d\" % warmup_iters)\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
    "\n",
    "    if sched_type == \"cos\":\n",
    "        iters = np.arange(epochs * niter_per_ep - warmup_iters)\n",
    "        schedule = np.array([\n",
    "            final_value + 0.5 * (base_value - final_value) * (1 + math.cos(math.pi * i / (len(iters)))) for i in iters])\n",
    "    elif sched_type == \"linear\":\n",
    "        schedule = np.linspace(base_value, final_value, epochs * niter_per_ep - warmup_iters)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    schedule = np.concatenate((warmup_schedule, schedule))\n",
    "\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_grad_norm_\u001b[39m(parameters, norm_type: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parameters, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m      3\u001b[0m         parameters \u001b[38;5;241m=\u001b[39m [parameters]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NativeScalerWithGradNormCount:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The implementation code is modified from Timm (https://github.com/huggingface/pytorch-image-models/tree/main/timm\n",
    "def get_parameter_groups(model, weight_decay=1e-5, skip_list=(), get_num_layer=None, get_layer_scale=None):\n",
    "    parameter_group_names = {}\n",
    "    parameter_group_vars = {}\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue  # frozen weights\n",
    "        if len(param.shape) == 1 or name.endswith(\".bias\") or name in skip_list:\n",
    "            group_name = \"no_decay\"\n",
    "            this_weight_decay = 0.\n",
    "        else:\n",
    "            group_name = \"decay\"\n",
    "            this_weight_decay = weight_decay\n",
    "        if get_num_layer is not None:\n",
    "            layer_id = get_num_layer(name)\n",
    "            group_name = \"layer_%d_%s\" % (layer_id, group_name)\n",
    "        else:\n",
    "            layer_id = None\n",
    "\n",
    "        if group_name not in parameter_group_names:\n",
    "            if get_layer_scale is not None:\n",
    "                scale = get_layer_scale(layer_id)\n",
    "            else:\n",
    "                scale = 1.\n",
    "\n",
    "            parameter_group_names[group_name] = {\n",
    "                \"weight_decay\": this_weight_decay,\n",
    "                \"params\": [],\n",
    "                \"lr_scale\": scale\n",
    "            }\n",
    "            parameter_group_vars[group_name] = {\n",
    "                \"weight_decay\": this_weight_decay,\n",
    "                \"params\": [],\n",
    "                \"lr_scale\": scale\n",
    "            }\n",
    "\n",
    "        parameter_group_vars[group_name][\"params\"].append(param)\n",
    "        parameter_group_names[group_name][\"params\"].append(name)\n",
    "    print(\"Param groups = %s\" % json.dumps(parameter_group_names, indent=2))\n",
    "    return list(parameter_group_vars.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(args, model, get_num_layer=None, get_layer_scale=None, filter_bias_and_bn=True, skip_list=None):\n",
    "    opt_lower = args['opt'].lower()\n",
    "    weight_decay = args['weight_decay']\n",
    "    if weight_decay and filter_bias_and_bn:\n",
    "        skip = {}\n",
    "        if skip_list is not None:\n",
    "            skip = skip_list\n",
    "        elif hasattr(model, 'no_weight_decay'):\n",
    "            skip = model.no_weight_decay()\n",
    "        parameters = get_parameter_groups(model, weight_decay, skip, get_num_layer, get_layer_scale)\n",
    "        weight_decay = 0.\n",
    "    else:\n",
    "        parameters = model.parameters()\n",
    "\n",
    "    opt_args = dict(lr=args['lr'], weight_decay=weight_decay)\n",
    "    if hasattr(args, 'opt_eps') and args['opt_eps'] is not None:\n",
    "        opt_args['eps'] = args['opt_eps']\n",
    "    if hasattr(args, 'opt_betas') and args['opt_betas'] is not None:\n",
    "        opt_args['betas'] = args['opt_betas']\n",
    "\n",
    "    if opt_lower == 'adamw':\n",
    "        optimizer = optim.AdamW(parameters, **opt_args)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer\")\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mixed_precision:str=\"fp16\", seed:int=42):\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Initialize accelerator\n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    #kwargs_handlers=[ddp_kwargs],\n",
    "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], mixed_precision=mixed_precision, log_with='wandb')\n",
    "    device = accelerator.device\n",
    "    \n",
    "    # Initializer loader \n",
    "    train_loader = create_loader(train_dataset, is_train=True, batch_size=config['batch_size'])\n",
    "    dev_loader = create_loader(dev_dataset, is_train=False, batch_size=config['test_batch_size'])\n",
    "    test_loader = create_loader(test_dataset, is_train=False, batch_size=256)\n",
    "    \n",
    "    # Initialize BEIPLUG model\n",
    "    with accelerator.main_process_first():\n",
    "        model = BEIPLUG(tokenizer=tokenizer, config=config)\n",
    "\n",
    "    model_without_ddp = model.module\n",
    "    skip_weight_decay_list = model.no_weight_decay()\n",
    "\n",
    "    # Compile optimizer\n",
    "    optimizer = create_optimizer(\n",
    "            config, model_without_ddp, skip_list=skip_weight_decay_list,\n",
    "            get_num_layer=None, \n",
    "            get_layer_scale=None)\n",
    "    \n",
    "    loss_scaler = NativeScalerWithGradNormCount()\n",
    "\n",
    "    # Prepare objects for accelerator \n",
    "    model, optimizer, train_loader, dev_loader, test_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, dev_loader, test_loader\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    total_batch_size = config['batch_size'] * config['update_freq'] * get_world_size()\n",
    "    num_traning_steps_per_epoch = len(train_dataset) // total_batch_size\n",
    "\n",
    "    # Initialize lr_scheduler\n",
    "    lr_scheduler = cosine_scheduler(\n",
    "        base_value=config['lr'], final_value=config['min_lr'], epochs=config['max_epoch'],\n",
    "        niter_per_ep=num_traning_steps_per_epoch, warmup_epochs=config['warmup_epochs'], warmup_steps=config['warmup_steps']\n",
    "    )\n",
    "\n",
    "    start_epoch = config['start_epoch']\n",
    "    max_epoch = config['max_epoch']\n",
    "        \n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    accelerator.print(f'Number of training parameters {n_parameters}')\n",
    "    accelerator.print(f\"Batch size {config['batch_size']}\")\n",
    "    accelerator.print('Start training')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize experiments tracker\n",
    "    accelerator.init_trackers('vqa-vlsp-task-visobert', config=config)\n",
    " \n",
    "    # training\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        start_steps = epoch * num_traning_steps_per_epoch\n",
    "        \n",
    "        # trainings\n",
    "        model.train() \n",
    "        model.zero_grad()\n",
    "        model.micro_steps = 0\n",
    "\n",
    "        for data_iter_step, (image, question, answer) in enumerate(train_loader):                \n",
    "                step = data_iter_step // config['update_freq']\n",
    "                global_step = start_steps + step\n",
    "\n",
    "                if lr_scheduler is not None and data_iter_step % config['update_freq'] == 0:\n",
    "                    for i, param_group in enumerate(optimizer.param_groups):\n",
    "                        if lr_scheduler is not None:\n",
    "                            param_group[\"lr\"] = lr_scheduler[global_step] * param_group[\"lr\"]\n",
    "                            \n",
    "                image = image.to(device, non_blocking=True)\n",
    "                question_input = tokenizer(question, padding='longest', truncation=True, return_tensors='pt').to(device)\n",
    "                answer_input = tokenizer(answer, padding='longest', return_tensors='pt').to(device)\n",
    "                training_loss = model(image, question_input, answer_input, train=True)\n",
    "\n",
    "                is_second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_second_order\n",
    "                training_loss /= config['update_freq']\n",
    "                grad_norm = loss_scaler(training_loss, optimizer, clip_grad=None,\n",
    "                                        parameters=model.parameters(), create_graph=is_second_order,\n",
    "                                        update_grad=(data_iter_step + 1) % config['update_freq'] == 0)\n",
    "                if (data_iter_step + 1) % config['update_freq'] == 0:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "        # evaluating\n",
    "        model.eval()\n",
    "        for image, question, answer in dev_loader:\n",
    "            with torch.no_grad():\n",
    "                image = image.to(device, non_blocking=True)\n",
    "                question_input = tokenizer(question, padding='longest', truncation=True, return_tensors='pt').to(device)\n",
    "                answer_input = tokenizer(answer, padding='longest', return_tensors='pt').to(device)\n",
    "                development_loss = model(image, question_input, answer_input, train=True)\n",
    "            accelerator.log({\"development_loss\": development_loss})\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch+1}: Training loss {training_loss} Evaluation loss {development_loss} .\")\n",
    "        \n",
    "        # Save model checkpoint after epoch\n",
    "        #accelerator.save_state(config['checkpoint_dir'])\n",
    "        \n",
    "    accelerator.print(\"End training\")\n",
    "    end_time = time.time()\n",
    "    accelerator.print(f\"Total time {start_time - end_time}\")\n",
    "    \n",
    "    # Save model\n",
    "    accelerator.wait_for_everyone()\n",
    "    model = accelerator.unwrap_model(model)\n",
    "    accelerator.save(model, config['save_model'])\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    set_seed(config['seed'])\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(log_with='wandb')\n",
    "    device = accelerator.device\n",
    "    \n",
    "    # Initialize BEIPLUG model\n",
    "    model = BEIPLUG(tokenizer=tokenizer, config=config)\n",
    "\n",
    "    # Compile optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=config['lr'])\n",
    "\n",
    "    # Prepare objects for accelerator \n",
    "    model, optimizer, train_loader, dev_loader, test_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, dev_loader, test_loader\n",
    "    )\n",
    "\n",
    "    # Initialize experiments tracker\n",
    "    accelerator.init_trackers()\n",
    "\n",
    "    total_batch_size = config['batch_size'] * config['update_freq'] * get_world_size()\n",
    "    num_traning_steps_per_epoch = len(train_loader) // total_batch_size\n",
    "\n",
    "    # Initialize lr_scheduler\n",
    "    lr_scheduler = cosine_scheduler(\n",
    "        base_value=config['lr'], final_value=config['min_lr'], epochs=config['max_epoch'],\n",
    "        niter_per_ep=num_traning_steps_per_epoch, warmup_epochs=config['warmup_epochs'], warmup_steps=config['warmup_steps']\n",
    "    )\n",
    "\n",
    "    start_epoch = config['start_epoch']\n",
    "    max_epoch = config['max_epoch']\n",
    "        \n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    accelerator.print(f'Number of training parameters {n_parameters}')\n",
    "    accelerator.print(f\"Batch size {config['batch_size']}\")\n",
    "    accelerator.print('Start training')\n",
    "    start_time = time.time()\n",
    "\n",
    "    start_steps = config['max_epoch'] * num_traning_steps_per_epoch\n",
    " \n",
    "    # training\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        # training\n",
    "        model.train(True)        \n",
    "        for data_iter_step, (image, question, answer) in enumerate(train_loader):\n",
    "            model.zero_grad()\n",
    "            model.micro_steps = 0   \n",
    "            step = data_iter_step // config['update_freq']\n",
    "            global_step = start_steps + step\n",
    "\n",
    "            if lr_scheduler is not None and data_iter_step % config['update_freq'] == 0:\n",
    "                for i, param_group in enumerate(optimizer.param_groups):\n",
    "                    if lr_scheduler is not None:\n",
    "                        param_group['lr'] = lr_scheduler[global_step] * param_group['lr_scale']\n",
    "\n",
    "            question_input = tokenizer(question, padding='longest', truncation=True, return_tensors='pt').to(device)\n",
    "            answer_input = tokenizer(answer, padding='longest', return_tensors='pt').to(device)\n",
    "            training_loss = model(image, question_input, answer_input, train=True)\n",
    "            accelerator.log({\"training_loss\": training_loss})\n",
    "            accelerator.backward(training_loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # evaluating\n",
    "        model.eval()\n",
    "        for image, question, answer in dev_loader:\n",
    "            with torch.no_grad():\n",
    "                question_input = tokenizer(question, padding='longest', truncation=True, return_tensors='pt').to(device)\n",
    "                answer_input = tokenizer(answer, padding='longest', return_tensors='pt').to(device)\n",
    "                development_loss = model(image, question_input, answer_input, train=True)\n",
    "            accelerator.log({\"development_loss\": development_loss})\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch+1}: Training loss {training_loss} Evaluation loss {development_loss} .\")\n",
    "        \n",
    "        # testing\n",
    "        if epoch >=10 and epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            all_predict_answer = {}\n",
    "            for image, question, question_ids in test_loader:\n",
    "                with torch.no_grad():\n",
    "                    question_input = tokenizer(question, padding='longest', truncation=True, return_tensors='pt').to(device)\n",
    "                    topk_ids, topk_probs = model(image=image, question=question_input, is_train=False)\n",
    "                topk_ids = accelerator.gather(topk_ids)\n",
    "                question_ids = accelerator.gather(question_ids)\n",
    "\n",
    "                for question_id, topk_id in zip(question_ids, topk_ids):\n",
    "                    predict_answer = tokenizer.decode(topk_id).replace(\"[SEP]\", \"\").replace(\"[CLS]\", \"\").replace(\"[PAD]\", \"\").strip()\n",
    "                    all_predict_answer[question_id] = predict_answer\n",
    "\n",
    "                \n",
    "        # Save model checkpoint after epoch\n",
    "        accelerator.save_state(config['checkpoint_dir'])\n",
    "\n",
    "\n",
    "    accelerator.print(\"End training\")\n",
    "    end_time = time.time()\n",
    "    accelerator.print(f\"Total time {start_time - end_time}\")\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(main, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
