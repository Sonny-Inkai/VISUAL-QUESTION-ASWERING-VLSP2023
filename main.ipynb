{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2022 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision import transforms\n",
    "\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.data.transforms import RandomResizedCropAndInterpolation\n",
    "from timm.data import create_transform\n",
    "from PIL import Image\n",
    "\n",
    "from tasks.randaugrandaug import RandomAugment\n",
    "\n",
    "from models.modeling_mplug import BertLMHeadModel, BertModel, BertConfig, FusionModel\n",
    "\n",
    "from transformers import AutoTokenizer, XLMRobertaTokenizer\n",
    "from models.predictor import TextGenerator\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEmbedding(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        contain_mask_token=False,\n",
    "        prepend_cls_token=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "        if contain_mask_token:\n",
    "            self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.mask_token = None\n",
    "\n",
    "        if prepend_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.cls_token = None\n",
    "\n",
    "    def num_position_embeddings(self):\n",
    "        if self.cls_token is None:\n",
    "            return self.num_patches\n",
    "        else:\n",
    "            return self.num_patches + 1\n",
    "\n",
    "    def forward(self, x, masked_position=None, **kwargs):\n",
    "        B, C, H, W = x.shape\n",
    "        assert (\n",
    "            H == self.img_size[0] and W == self.img_size[1]\n",
    "        ), f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        if masked_position is not None:\n",
    "            assert self.mask_token is not None\n",
    "            mask_token = self.mask_token.expand(batch_size, seq_len, -1)\n",
    "            w = masked_position.unsqueeze(-1).type_as(mask_token)\n",
    "            x = x * (1 - w) + mask_token * w\n",
    "\n",
    "        if self.cls_token is not None:\n",
    "            cls_tokens = self.cls_token.expand(\n",
    "                batch_size, -1, -1\n",
    "            )  # stole cls_tokens impl from Phil Wang, thanks\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transform(is_train, args):\n",
    "    if is_train:\n",
    "        t = [\n",
    "            RandomResizedCropAndInterpolation(args[\"input_size\"], scale=(0.5, 1.0), interpolation=args[\"train_interpolation\"]), \n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ]\n",
    "        if args[\"randaug\"]:\n",
    "            t.append(\n",
    "                RandomAugment(\n",
    "                    2, 7, isPIL=True, \n",
    "                    augs=[\n",
    "                        'Identity','AutoContrast','Equalize','Brightness','Sharpness', \n",
    "                        'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Rotate', \n",
    "                    ]))\n",
    "        t += [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD), \n",
    "        ]\n",
    "        t = transforms.Compose(t)\n",
    "    else:\n",
    "        t = transforms.Compose([\n",
    "            transforms.Resize((args[\"input_size\"], args[\"input_size\"]), interpolation=3), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)\n",
    "        ])\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        'task': None,\n",
    "        'input_size': 224,\n",
    "        'train_interpolation': 'bicubic',\n",
    "        'randaug': True,\n",
    "        'text_encoder': 'uitnlp/CafeBERT',\n",
    "        'text_decoder': 'uitnlp/CafeBERT',\n",
    "        'vision_width': 768,\n",
    "        'roberta_config': '.\\\\configs\\\\config_bert.json',\n",
    "        'beam_size': 5,\n",
    "        'min_length': 1,\n",
    "        'max_length': 10,\n",
    "        'start_epoch': 0,\n",
    "        'max_epoch': 10, # default 20\n",
    "        'batch_size': 2, # default 128\n",
    "        'seed': 42,\n",
    "        'lr': 5e-4,\n",
    "        'min_lr': 1e-6,\n",
    "        'warmup_epochs': 5,\n",
    "        'warmup_steps': -1,\n",
    "        'update_freq': 1,\n",
    "        'checkpoint_dir': '..\\\\save_states',\n",
    "        'eos': '[SEP]'\n",
    "             \n",
    "    }\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEIPLUG(nn.Module):\n",
    "    def __init__(self, tokenizer: None, config: None) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.module_setting(config)\n",
    "        self.visual_encoder = VisionEmbedding(contain_mask_token=True, prepend_cls_token=True)\n",
    "        self.text_encoder = BertModel.from_pretrained(config['text_encoder'], config=self.config_encoder, add_pooling_layer=False)\n",
    "        self.fusion_encoder = FusionModel.from_pretrained(config['text_encoder'], config=self.config_fusion, add_pooling_layer=False)\n",
    "        self.text_decoder = BertLMHeadModel.from_pretrained(config['text_decoder'], config=self.config_decoder)\n",
    "        self.beam_generator = TextGenerator(args=config, model=self.text_decoder)\n",
    "\n",
    "    def forward(self, image: None, question: None, answer: None, train: True):\n",
    "        image = image.to(dtype=next(self.parameters()).dtype)\n",
    "        image_embeds = self.visual_encoder(image)\n",
    "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.divice)\n",
    "\n",
    "        if train:\n",
    "            answer_target = answer.input_ids.masked_fill(answer.input_ids == self.tokenizer.pad_token_id, -100)\n",
    "            text_output = self.text_encoder(input_ids=question.input_ids, attention_mask=question.attention_mask, return_dict=True)\n",
    "            text_embeds = text_output.last_hidden_state\n",
    "            fusion_output = self.fusion_encoder.forward(\n",
    "                encoder_embeds=text_embeds,\n",
    "                attention_mask=question.attention_mask,\n",
    "                encoder_hidden_states=image_embeds,\n",
    "                encoder_attention_mask=image_atts,\n",
    "                return_dict=False\n",
    "            )\n",
    "\n",
    "            image_output, question_output = fusion_output\n",
    "\n",
    "            question_output = torch.cat([image_output, question_output], dim=1)\n",
    "            merge_text_attention = torch.cat([image_atts, question.attention_mask], d√≠m=1)\n",
    "\n",
    "            answer_output = self.text_decoder.forward(input_ids=answer.input_ids,\n",
    "                                                    attention_mask=answer.attention_mask,\n",
    "                                                    encoder_hidden_states=question_output,\n",
    "                                                    encoder_attention_mask=merge_text_attention,\n",
    "                                                    labels=answer_target,\n",
    "                                                    return_dict=True,\n",
    "                                                    reduction='none'\n",
    "            )\n",
    "\n",
    "            loss = answer_output.loss\n",
    "            loss = loss.sum()/image.size(0)\n",
    "\n",
    "            return loss\n",
    "\n",
    "        else:\n",
    "            text_output = self.text_decoder(question.input_ids, attetion_mask=question.attention_mask, return_dict=True)\n",
    "            text_embeds = text_output.last_hidden_state\n",
    "            fusion_output = self.fusion_encoder.forward(\n",
    "                encoder_embeds=text_embeds,\n",
    "                attention_mask=question.attention_mask,\n",
    "                encoder_hidden_states=image_embeds,\n",
    "                encoder_attention_mask=image_atts,\n",
    "                return_dict=False\n",
    "            )\n",
    "\n",
    "            image_output, question_output = fusion_output\n",
    "            question_output = torch.cat([image_output, question_output], dim=1)\n",
    "            merge_text_attention = torch.cat([image_atts, question.attention_mask], dim=1)\n",
    "            topk_ids, topk_probs = self.generation(question_states=question_output, question_atts=merge_text_attention)\n",
    "\n",
    "            return topk_ids, topk_probs\n",
    "\n",
    "\n",
    "\n",
    "    def module_setting(self, config):\n",
    "        self.config_encoder = BertConfig.from_json_file(config['roberta_config'])   \n",
    "        self.config_encoder.num_hidden_layers = self.config_encoder.text_encoder_layers\n",
    "        self.config_fusion = BertConfig.from_json_file(config['roberta_config'])   \n",
    "        self.config_decoder = BertConfig.from_json_file(config['roberta_config'])\n",
    "        self.config_decoder.add_cross_attention = True\n",
    "        self.config_decoder.num_hidden_layers = self.config_decoder.text_decode_layers\n",
    "\n",
    "    def generation(self, question_states, question_atts):\n",
    "        encoder_inputs = [question_states, question_atts]\n",
    "        topk_ids, topk_scores = self.beam_generator.translate_batch(encoder_inputs)  \n",
    "        return topk_ids, topk_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_path: None, image_path: None, is_train: bool, config: None) -> None:\n",
    "        super().__init__()\n",
    "        self.is_train = is_train\n",
    "        self.image_transform = build_transform(is_train=is_train, args=config)\n",
    "        self.dataset = json.load(open(data_path, 'r'))\n",
    "        self.question_list = list(self.dataset['annotations'].keys())\n",
    "        self.image_path = image_path\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.question_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            question_id = self.question_list[idx]\n",
    "            set_vqa_dict = self.dataset['annotations'][question_id]            \n",
    "            question = set_vqa_dict['question']\n",
    "            answer = set_vqa_dict['answer']\n",
    "            \n",
    "            # Mapping to get image\n",
    "            image_id = set_vqa_dict['image_id']\n",
    "            image_id_path = self.dataset['images'][str(image_id)]\n",
    "            image_path = self.image_path + \"\\\\\" + image_id_path\n",
    "            image = default_loader(image_path)\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "            return image, question, answer\n",
    "    \n",
    "        else:\n",
    "            question_id = self.question_list[idx]\n",
    "            set_vqa_dict = self.dataset['annotations'][question_id]            \n",
    "            question = set_vqa_dict['question']\n",
    "                    \n",
    "            # Mapping to get image\n",
    "            image_id = set_vqa_dict['image_id']\n",
    "            image_id_path = self.dataset['images'][str(image_id)]\n",
    "            image_path = self.image_path + \"\\\\\" + image_id_path\n",
    "            image = default_loader(image_path)\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "            return image, question, question_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader(dataset: Dataset, is_train: bool, batch_size: int, num_workers: int = 0):\n",
    "    if is_train:\n",
    "        shuffle = True\n",
    "        drop_last = True\n",
    "    else:        \n",
    "        shuffle = False\n",
    "        drop_last = False\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=drop_last\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Accelerator notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \".\\\\data\\\\\"\n",
    "train_dataset = Dataset(data_path + \"training-data\\\\vlsp2023_train_data.json\", \n",
    "                        data_path + \"training-data\\\\training-images\",\n",
    "                        is_train=True, config=config\n",
    "                        )\n",
    "dev_dataset = Dataset(data_path + \"public-test-data\\\\vlsp2023_dev_data.json\", \n",
    "                        data_path + \"public-test-data\\\\dev-images\",\n",
    "                        is_train=False, config=config\n",
    "                        )\n",
    "test_dataset = Dataset(data_path + \"private-test-data\\\\vlsp2023_test_data.json\", \n",
    "                        data_path + \"private-test-data\\\\test-images\",\n",
    "                        is_train=False, config=config\n",
    "                        )\n",
    "\n",
    "train_loader = create_loader(train_dataset, is_train=True, batch_size=config['batch_size'])\n",
    "dev_loader = create_loader(dev_dataset, is_train=False, batch_size=config['batch_size'])\n",
    "test_loader = create_loader(test_dataset, is_train=False, batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_encoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Theba\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:837\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    834\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    835\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    836\u001b[0m         )\n\u001b[1;32m--> 837\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    839\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mc:\\Users\\Theba\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2086\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2083\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2084\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2087\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2088\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2089\u001b[0m     init_configuration,\n\u001b[0;32m   2090\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2091\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2092\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2093\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2094\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2095\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2096\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2097\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2098\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Theba\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2325\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2329\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2330\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Theba\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\models\\xlm_roberta\\tokenization_xlm_roberta_fast.py:155\u001b[0m, in \u001b[0;36mXLMRobertaTokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    141\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m ):\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     mask_token \u001b[38;5;241m=\u001b[39m AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    156\u001b[0m         vocab_file,\n\u001b[0;32m    157\u001b[0m         tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[0;32m    158\u001b[0m         bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    159\u001b[0m         eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[0;32m    160\u001b[0m         sep_token\u001b[38;5;241m=\u001b[39msep_token,\n\u001b[0;32m    161\u001b[0m         cls_token\u001b[38;5;241m=\u001b[39mcls_token,\n\u001b[0;32m    162\u001b[0m         unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[0;32m    163\u001b[0m         pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[0;32m    164\u001b[0m         mask_token\u001b[38;5;241m=\u001b[39mmask_token,\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    166\u001b[0m     )\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n",
      "File \u001b[1;32mc:\\Users\\Theba\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m     )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['text_encoder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scheduler lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0,\n",
    "                     start_warmup_value=0, warmup_steps=-1, sched_type=\"cos\"):\n",
    "    warmup_schedule = np.array([])\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    if warmup_steps > 0:\n",
    "        warmup_iters = warmup_steps\n",
    "    #print(\"Set warmup steps = %d\" % warmup_iters)\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
    "\n",
    "    if sched_type == \"cos\":\n",
    "        iters = np.arange(epochs * niter_per_ep - warmup_iters)\n",
    "        schedule = np.array([\n",
    "            final_value + 0.5 * (base_value - final_value) * (1 + math.cos(math.pi * i / (len(iters)))) for i in iters])\n",
    "    elif sched_type == \"linear\":\n",
    "        schedule = np.linspace(base_value, final_value, epochs * niter_per_ep - warmup_iters)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    schedule = np.concatenate((warmup_schedule, schedule))\n",
    "\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    set_seed(config['seed'])\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(log_with='wandb')\n",
    "    device = accelerator.device\n",
    "    \n",
    "    # Initialize BEIPLUG model\n",
    "    model = BEIPLUG(tokenizer=tokenizer, config=config)\n",
    "\n",
    "    # Compile optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=config['lr'])\n",
    "\n",
    "    # Prepare objects for accelerator \n",
    "    model, optimizer, train_loader, dev_loader, test_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, dev_loader, test_loader\n",
    "    )\n",
    "\n",
    "    # Initialize experiments tracker\n",
    "    accelerator.init_trackers()\n",
    "\n",
    "    total_batch_size = config['batch_size'] * config['update_freq'] * get_world_size()\n",
    "    num_traning_steps_per_epoch = len(train_loader) // total_batch_size\n",
    "\n",
    "    # Initialize lr_scheduler\n",
    "    lr_scheduler = cosine_scheduler(\n",
    "        base_value=config['lr'], final_value=config['min_lr'], epochs=config['max_epoch'],\n",
    "        niter_per_ep=num_traning_steps_per_epoch, warmup_epochs=config['warmup_epochs'], warmup_steps=config['warmup_steps']\n",
    "    )\n",
    "\n",
    "    start_epoch = config['start_epoch']\n",
    "    max_epoch = config['max_epoch']\n",
    "        \n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    accelerator.print(f'Number of training parameters {n_parameters}')\n",
    "    accelerator.print(f\"Batch size {config['batch_size']}\")\n",
    "    accelerator.print('Start training')\n",
    "    start_time = time.time()\n",
    "\n",
    "    start_steps = config['max_epoch'] * num_traning_steps_per_epoch\n",
    " \n",
    "    # training\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        # training\n",
    "        model.train(True)        \n",
    "        for data_iter_step, (image, question, answer) in enumerate(train_loader):\n",
    "            model.zero_grad()\n",
    "            model.micro_steps = 0   \n",
    "            step = data_iter_step // config['update_freq']\n",
    "            global_step = start_steps + step\n",
    "\n",
    "            if lr_scheduler is not None and data_iter_step % config['update_freq'] == 0:\n",
    "                for i, param_group in enumerate(optimizer.param_groups):\n",
    "                    if lr_scheduler is not None:\n",
    "                        param_group['lr'] = lr_scheduler[global_step] * param_group['lr_scale']\n",
    "\n",
    "            question_input = tokenizer(question, padding='longest', truncation=True, return_tensors='pt').to(device)\n",
    "            answer_input = tokenizer(answer, padding='longest', return_tensors='pt').to(device)\n",
    "            training_loss = model(image, question_input, answer_input, train=True)\n",
    "            accelerator.log({\"training_loss\": training_loss})\n",
    "            accelerator.backward(training_loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # evaluating\n",
    "        model.eval()\n",
    "        for image, question, answer in dev_loader:\n",
    "            with torch.no_grad():\n",
    "                question_input = tokenizer(question, padding='longest', truncation=True, return_tensors='pt').to(device)\n",
    "                answer_input = tokenizer(answer, padding='longest', return_tensors='pt').to(device)\n",
    "                development_loss = model(image, question_input, answer_input, train=True)\n",
    "            accelerator.log({\"development_loss\": development_loss})\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch+1}: Training loss {training_loss} Evaluation loss {development_loss} .\")\n",
    "        \n",
    "        # testing\n",
    "        if epoch >=10 and epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            all_predict_answer = {}\n",
    "            for image, question, question_ids in test_loader:\n",
    "                with torch.no_grad():\n",
    "                    question_input = tokenizer(question, padding='longest', truncation=True, return_tensors='pt').to(device)\n",
    "                    topk_ids, topk_probs = model(image=image, question=question_input, is_train=False)\n",
    "                topk_ids = accelerator.gather(topk_ids)\n",
    "                question_ids = accelerator.gather(question_ids)\n",
    "\n",
    "                for question_id, topk_id in zip(question_ids, topk_ids):\n",
    "                    predict_answer = tokenizer.decode(topk_id).replace(\"[SEP]\", \"\").replace(\"[CLS]\", \"\").replace(\"[PAD]\", \"\").strip()\n",
    "                    all_predict_answer[question_id] = predict_answer\n",
    "\n",
    "                \n",
    "        # Save model checkpoint after epoch\n",
    "        accelerator.save_state(config['checkpoint_dir'])\n",
    "\n",
    "\n",
    "    accelerator.print(\"End training\")\n",
    "    end_time = time.time()\n",
    "    accelerator.print(f\"Total time {start_time - end_time}\")\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(main, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
